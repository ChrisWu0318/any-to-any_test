import gradio as gr
import torch
from diffusers import StableDiffusionPipeline
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import WhisperProcessor, WhisperForConditionalGeneration
import torchaudio
import numpy as np

device = torch.device("mps")  # Mac M4 ä½¿ç”¨ MPS åç«¯

# === Text â†’ Image ===
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5")
pipe = pipe.to(device)

# === Image â†’ Text ===
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

# === Audio â†’ Text ===
asr_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
asr_model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small").to("mps")


# Text â†’ Image
def text_to_image(prompt):
    image = pipe(prompt).images[0]
    return image

# Image â†’ Text
def image_to_text(image):
    inputs = processor(images=image, return_tensors="pt").to(device)
    generated_ids = model.generate(**inputs)
    caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return caption

# Audio â†’ Text
def audio_to_text(audio):
    if audio is None:
        return "âŒ æ²¡æœ‰éŸ³é¢‘è¾“å…¥"
    sr, waveform = audio  # æ³¨æ„é¡ºåºï¼

    print("waveform:", type(waveform), waveform.shape if hasattr(waveform, 'shape') else None, "sr:", sr, type(sr))

    # é‡‡æ ·ç‡åˆ¤å®š
    try:
        sr_val = int(sr)
    except Exception as e:
        return f"âŒ æ— æ³•è¯†åˆ«éŸ³é¢‘é‡‡æ ·ç‡: {e}"

    if not isinstance(sr_val, int) or sr_val <= 0:
        return "âŒ éŸ³é¢‘é‡‡æ ·ç‡æ— æ•ˆ"

    # å¤„ç†ç«‹ä½“å£°ï¼ˆ(2, n)ï¼‰ï¼Œè½¬å•å£°é“ï¼ˆ(n,)ï¼‰
    if isinstance(waveform, np.ndarray) and len(waveform.shape) == 2:
        waveform = waveform.mean(axis=0)

    # ä¿è¯ float32
    waveform = waveform.astype(np.float32)

    # whisper è¦æ±‚é‡‡æ ·ç‡ 16000
    if sr_val != 16000:
        waveform_torch = torch.tensor(waveform, dtype=torch.float32)
        waveform_torch = waveform_torch.unsqueeze(0) if len(waveform_torch.shape) == 1 else waveform_torch
        waveform_resampled = torchaudio.functional.resample(waveform_torch, sr_val, 16000)
        waveform = waveform_resampled.squeeze().cpu().numpy()

    # ç›´æ¥ä¼  numpy æ•°ç»„ç»™ asr_processor
    inputs = asr_processor(waveform, sampling_rate=16000, return_tensors="pt").to(device)
    # å¼ºåˆ¶è‹±æ–‡è¯†åˆ«
    forced_decoder_ids = asr_processor.get_decoder_prompt_ids(language="en", task="transcribe")
    predicted_ids = asr_model.generate(inputs["input_features"], forced_decoder_ids=forced_decoder_ids)
    text = asr_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return text


# === Gradio UI ===
with gr.Blocks() as demo:
    gr.Markdown("## ğŸŒŸ Any-to-Any å¤šæ¨¡æ€ç³»ç»Ÿï¼ˆæ”¯æŒ Text/Image/Audio äº’è½¬ï¼‰")

    with gr.Tab("Text â†’ Image"):
        prompt_input = gr.Textbox(label="è¾“å…¥æ–‡æœ¬æè¿°")
        image_output = gr.Image()
        btn_img = gr.Button("ç”Ÿæˆå›¾åƒ")
        btn_img.click(text_to_image, prompt_input, image_output)

    with gr.Tab("Image â†’ Text"):
        image_input = gr.Image()
        text_output = gr.Textbox(label="å›¾ç‰‡å†…å®¹æè¿°")
        btn_txt = gr.Button("ç”Ÿæˆæè¿°")
        btn_txt.click(image_to_text, image_input, text_output)

    with gr.Tab("Audio â†’ Text"):
        audio_input = gr.Audio(type="numpy", label="ä¸Šä¼ éŸ³é¢‘æˆ–å½•éŸ³")
        audio_text_output = gr.Textbox(label="è¯­éŸ³è¯†åˆ«ç»“æœ")
        btn_asr = gr.Button("è½¬ä¸ºæ–‡å­—")
        btn_asr.click(audio_to_text, audio_input, audio_text_output)

demo.launch()
